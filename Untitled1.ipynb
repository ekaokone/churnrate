{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Context\n",
    "\n",
    "\"Predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\"\n",
    "\n",
    "\n",
    "# Dataset Content\n",
    "\n",
    "Each row represents a customer, each column contains customer’s attributes described on the column Metadata.\n",
    "\n",
    "\n",
    "The data set includes information about:\n",
    "\n",
    " - Customers who left within the last month – the column is called Churn\n",
    " - Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    " \n",
    " - Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
    " \n",
    "- Demographic info about customers – gender, age range, and if they have partners and dependents\n",
    "\n",
    "\n",
    "\n",
    "# Inspiration\n",
    "\n",
    "To explore this type of models and learn more about the subject.\n",
    "\n",
    "#Importing libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, manuipulation and wrangling\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import seaborn as sns # visualization\n",
    "\n",
    "import plotly.graph_objs as go # visualization\n",
    "import plotly.express as px # visualization\n",
    "import plotly.figure_factory as ff # visualization\n",
    "import plotly.offline as py # visualization\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "\n",
    "# Reading the dataset\n",
    "data = pd.read_csv('../dataset/churn.csv')\n",
    "\n",
    "# Get the first five instance of the dataset\n",
    "data.head()\n",
    "\n",
    "### Data Overview\n",
    "\n",
    "# Function to get quick overview\n",
    "def dataoverview(df):\n",
    "    print('Overview of the dataset: \\n')\n",
    "    print(\"Rows:\", df.shape[0])\n",
    "    print(\"\\nNumber of features:\", df.shape[1])\n",
    "    print(\"\\nFeatures:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nUnique values:\\n\")\n",
    "    print(df.nunique())\n",
    "    print(\"\\nDescriptive statistics overview\")\n",
    "    print(df.describe())\n",
    "\n",
    "dataoverview(data)\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "## “Garbage in, Garbage out.” — George Fuechsel\n",
    "\n",
    "Data Preprocessing refers to the steps applied to make data more suitable for data mining(which are just procedures to gain insights from data such as data analytics, machine learning etc.). These are techniques that are used to convert the raw data into a clean data set.\n",
    "\n",
    "\n",
    "In a real-world data science project, data preprocessing is one of the most important things, and it is one of the common factors of success of a model. It is the longest dtep in the predictive modelling workflow.\n",
    "\n",
    "Basic data preprocessing steps includes but not limited to:\n",
    "- Formatting data types\n",
    "- Handling missing values\n",
    "- Handling irrelevant/redundant features\n",
    "- Handling categorical data\n",
    "- Feature scaling (Normalization)\n",
    "- splitting the dataset into training and testing datasets\n",
    "\n",
    "#### Formatting data types\n",
    "\n",
    "To start, let’s define what data types exist and what measurement scales they have:\n",
    "1. Numeric\n",
    "    - Discrete\n",
    "    - Continous\n",
    "2. Categorical\n",
    "    - Ordinal\n",
    "    - Nominal\n",
    "    - Binary\n",
    "3. Date\n",
    "4. Time series\n",
    "5. Text\n",
    "6. Images\n",
    "\n",
    "data.info() \n",
    "\n",
    "#Or this\n",
    "#data.dtypes\n",
    "\n",
    "data['TotalCharges'] = data['TotalCharges'].replace(r'\\s+', np.nan, regex=True)\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])\n",
    "\n",
    "data.info()\n",
    "\n",
    "#### Handling missing values\n",
    "\n",
    "\"Missing values are common occurrences in data. Unfortunately, most predictive modeling techniques cannot handle any missing values. Therefore, this problem must be addressed prior to modeling.\" — Page 203, Feature Engineering and Selection, 2019.\n",
    "\n",
    "Missing values can take various forms such as np.nan, null, None, NaN etc. \n",
    "\n",
    "We can deal these missing values in two forms:\n",
    "1. Removing missing values\n",
    "2. Replacing missing values\n",
    "\n",
    "The rational behind the respective options is the amount of the missing values. \n",
    "\n",
    "```python\n",
    "\n",
    "if missing values is many:\n",
    "    drop\n",
    "else:\n",
    "    replace\n",
    "```\n",
    "\n",
    "- We can drop a row or column with missing values using pandas dropna() function (set a threshold for missing values in order for a row/column to be dropped)\n",
    "- fillna() function of Pandas conveniently handles missing values. Using fillna(), missing values can be replaced by a special value or an aggreate value such as mean, median. Furthermore, missing values can be replaced with the value before or after it which is pretty useful for time-series datasets.\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "# Checking for the distribution of the feature\n",
    "data['TotalCharges'].hist(figsize=(10,10));\n",
    "\n",
    "# Due to the skewness, we will be utilizing the median as the central tendency of the data\n",
    "median = data['TotalCharges'].median()\n",
    "data['TotalCharges'].fillna(median, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "data.head()\n",
    "\n",
    "#### Let's look at customers who churn within the last month\n",
    "\n",
    "# Distribution in percentages\n",
    "trace = go.Pie(labels = data[\"Churn\"].value_counts().keys().tolist(),\n",
    "               values = data[\"Churn\"].value_counts().values.tolist(),\n",
    "               marker = dict(colors=['seagreen', 'indianred'],\n",
    "                   line = dict(color = \"white\", width =  1.3)),\n",
    "               rotation = 90,\n",
    "               hoverinfo = \"label+value+text\",\n",
    "               hole = .5\n",
    "              )\n",
    "layout = go.Layout(dict(title = \"Churn (Target) Distribution within the last month (%)\",\n",
    "                \n",
    "                       )\n",
    "                  )\n",
    "trace = [trace]\n",
    "fig = go.Figure(data = trace, layout = layout)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Distribution in counts\n",
    "trace = go.Bar(\n",
    "    x=data.groupby('Churn')['customerID'].count().index,\n",
    "    y=data.groupby('Churn')['customerID'].count().values,\n",
    "    marker=dict(\n",
    "        color=['seagreen', 'indianred']),)\n",
    "\n",
    "trace = [trace]\n",
    "layout = go.Layout(\n",
    "    title='Churn (Target) Distribution', \n",
    "    xaxis=dict(\n",
    "        title='Customer Churn?'),\n",
    "    yaxis=dict(\n",
    "        title='Count')\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "In our data, 74% of the customers do not churn. Clearly the data is skewed as we would expect a large majority of the customers to not churn. This is important to keep in mind for our modelling as skeweness could lead to a lot of false negatives. We will see in the modelling section on how to avoid skewness in the data.\n",
    "\n",
    "data.info()\n",
    "\n",
    "Let's consider some numerical columns with relation to the churn. The features to be considered are:\n",
    "- tenure\n",
    "- monthly charges\n",
    "- total charges\n",
    "\n",
    "Let's look at the probability density distribution, which is use to pecify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value.\n",
    "\n",
    "We will be utilizing Seaborn Kdeplot (Kernel Distribution Estimation Plot) which depicts the probability density function of the continuous or non-parametric data variables\n",
    "\n",
    "def kdeplot(feature):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\"KDE for {}\".format(feature))\n",
    "    ax0 = sns.kdeplot(data[data['Churn'] == 'No'][feature].dropna(), color= 'seagreen', label= 'Churn: No')\n",
    "    ax1 = sns.kdeplot(data[data['Churn'] == 'Yes'][feature].dropna(), color= 'indianred', label= 'Churn: Yes')\n",
    "\n",
    "kdeplot('tenure')\n",
    "kdeplot('MonthlyCharges')\n",
    "kdeplot('TotalCharges')\n",
    "\n",
    "From the plots above we can conclude that:\n",
    "- Recent clients are more likely to churn\n",
    "- Clients with higher MonthlyCharges are also more likely to churn\n",
    "- Tenure and MonthlyCharges are probably important features\n",
    "\n",
    "Let's look at scatterplots of the numeric features\n",
    "\n",
    "def scatterplot(feature):\n",
    "    fig = px.scatter(data, x=feature, y=\"tenure\",color=\"Churn\",\n",
    "                 color_discrete_sequence=['seagreen', 'indianred'])\n",
    "    fig.show()\n",
    "\n",
    "scatterplot('MonthlyCharges')\n",
    "scatterplot('TotalCharges')\n",
    "\n",
    "When we look at churner data, total charges tend to increase with tenure and this could have been a factor for churning. When we look at the same for non-churners, there doesn’t seem to be a clear increasing trend.\n",
    "\n",
    "Let's consider some categorical columns with relation to the churn.\n",
    "\n",
    "### Demographics:\n",
    "\n",
    "Let's first understand the\n",
    "- gender, \n",
    "- Senior citizen (Young and Old), \n",
    "- partner and \n",
    "- dependent status of the customers.\n",
    "\n",
    "def demographics_analysis(data):\n",
    "    demographics = ['gender','SeniorCitizen', 'Partner', 'Dependents']\n",
    "    for demographic in demographics:\n",
    "        \n",
    "        if demographic =='SeniorCitizen':\n",
    "            #Converting the predictor variable in a binary numeric variable\n",
    "            data[demographic].replace(to_replace=1, value='Yes', inplace=True)\n",
    "            data[demographic].replace(to_replace=0,  value='No', inplace=True)\n",
    "           \n",
    "            \n",
    "            #Feature distribution\n",
    "            df1 = data.groupby([demographic]).count()[['customerID']].reset_index()\n",
    "            df1['percentage'] = (df1['customerID']/df1['customerID'].values.sum())*100\n",
    "            fig = px.bar(df1, x=demographic, y='percentage', color=demographic)\n",
    "            fig.update_layout(title=f\"{demographic} distribution\",barmode='group')\n",
    "            fig.show()\n",
    "            \n",
    "            \n",
    "            #Feature distribution wrt Churn\n",
    "            df2 = data.groupby([demographic, 'Churn']).count()['customerID'].unstack()\n",
    "            \n",
    "            fig = go.Figure(data=[\n",
    "                go.Bar(name='Churn-Yes', x=df2.index, y=df2['Yes'], marker_color='indianred' ),\n",
    "                go.Bar(name='Churn-No', x=df2.index, y=df2['No'], marker_color='seagreen')])\n",
    "            \n",
    "            fig.update_layout( title=f\"{demographic} distribution wrt Churn\",barmode='group')\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #Feature distribution\n",
    "            df1 = data.groupby([demographic]).count()[['customerID']].reset_index()\n",
    "            df1['percentage'] = (df1['customerID']/df1['customerID'].values.sum())*100\n",
    "            fig = px.bar(df1, x=demographic, y='percentage', color=demographic)\n",
    "            fig.update_layout(title=f\"{demographic} distribution\",barmode='group')\n",
    "            fig.show()\n",
    "            \n",
    "            \n",
    "            #Feature distribution wrt Churn\n",
    "            df2 = data.groupby([demographic, 'Churn']).count()['customerID'].unstack()\n",
    "            \n",
    "            fig = go.Figure(data=[\n",
    "                go.Bar(name='Churn-Yes', x=df2.index, y=df2['Yes'], marker_color='indianred'),\n",
    "                go.Bar(name='Churn-No', x=df2.index, y=df2['No'],marker_color='seagreen')])\n",
    "            fig.update_layout( title=f\"{demographic} distribution wrt Churn\",barmode='group')\n",
    "#             fig.update_traces(marker_color='green')\n",
    "            fig.show()\n",
    "\n",
    "demographics_analysis(data)\n",
    "\n",
    "- Gender: The dataset contain around 7043 records distributed between females and males. The following diagrame shows the ratios for both in referrence to the \"Churn\".Its clarify that churn almost equaly distributed between both males and females. Therefor, it's considered has insignificint effect to the target variables.\n",
    "\n",
    "- Senior citizen: feature considered an important variable to predict the churn. The following plots shows that the senior people who churns are double than who dont.\n",
    "\n",
    "- Dependents: Based on the above plot, it looks like that people dont churn if they have dependents. Hence, the dependents feature might have a significient contribution in predicting the \"Churn\".\n",
    "\n",
    "- Partner: Based on the above plot, it looks like that people dont churn if they have partners. Hence, the partner feature might have a significient contribution in predicting the \"Churn\".\n",
    "\n",
    "### Customer Account Information\n",
    "\n",
    "- Tenure\n",
    "- Contract Type\n",
    "\n",
    "tenure_df = data.groupby(['tenure']).count()[['customerID']].reset_index()\n",
    "fig = px.histogram(tenure_df, x=\"tenure\", y=\"customerID\")\n",
    "fig.show()\n",
    "\n",
    "After looking at the above histogram we can see that a lot of customers have been with the telecom company for 0-9 months. This could be potentially because different customers have different contracts. Thus based on the contract they are into it could be more/less easier for the customers to stay/leave the telecom company.\n",
    "\n",
    "#Contract Type\n",
    "contract_df = data.groupby(['Contract']).count()[['customerID']].reset_index()\n",
    "fig = px.bar(contract_df, x=\"Contract\", y='customerID', color=\"Contract\")\n",
    "fig.show()\n",
    "\n",
    "It can be seen that month of the customers prefer a monthly subscription plan.\n",
    "\n",
    "#Contract Type\n",
    "contract_tenure_df = data.groupby(['Contract', 'tenure']).count()[['customerID']].reset_index()\n",
    "# #Convertin the predictor variable in a binary numeric variable\n",
    "fig = px.histogram(contract_tenure_df, x=\"tenure\", y='customerID' ,color='Contract')\n",
    "fig.show()\n",
    "# contract_tenure_df.head()\n",
    "\n",
    "Interestingly most of the monthly contracts last for 1-2 months, while the one year contract tend to last for 50-54 months and lastly the 2 year contracts tend to last for about 70 months. This shows that the customers taking a longer contract are more loyal to the company and tend to stay with it for a longer period of time. \n",
    "\n",
    "data.columns\n",
    "\n",
    "def customer_services(data):\n",
    "    services = ['PhoneService','MultipleLines','InternetService','OnlineSecurity',\n",
    "           'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n",
    "    for service in services:\n",
    "        #Feature distribution\n",
    "        df1 = data.groupby([service]).count()[['customerID']].reset_index()\n",
    "        df1['percentage'] = (df1['customerID']/df1['customerID'].values.sum())*100\n",
    "        fig = px.bar(df1, x=service, y='percentage', color=service)\n",
    "        fig.update_layout(title=f\"{service} distribution\",barmode='group')\n",
    "        fig.show()\n",
    "\n",
    "        #Feature distribution wrt Churn\n",
    "        df2 = data.groupby([service, 'Churn']).count()['customerID'].unstack()\n",
    "\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(name='Churn-Yes', x=df2.index, y=df2['Yes'], marker_color='indianred' ),\n",
    "            go.Bar(name='Churn-No', x=df2.index, y=df2['No'], marker_color='seagreen')])\n",
    "\n",
    "        fig.update_layout( title=f\"{service} distribution wrt Churn\",barmode='group')\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "customer_services(data)\n",
    "\n",
    "We can interpret that the customers with Phone Service, Internet service been supplied by Fibre cables, no Online security and backup, no device protection, no technical support will most probably churn while those who have multiple lines, that stream TV and movies are approxiamately even distributed.\n",
    "\n",
    "data.head()\n",
    "\n",
    "### Payments\n",
    "\n",
    "def payment_services(data):\n",
    "    payment_services = ['PaymentMethod','PaperlessBilling']\n",
    "    for service in payment_services:\n",
    "            #Feature distribution\n",
    "            df1 = data.groupby([service]).count()[['customerID']].reset_index()\n",
    "            df1['percentage'] = (df1['customerID']/df1['customerID'].values.sum())*100\n",
    "            fig = px.bar(df1, x=service, y='percentage', color=service)\n",
    "            fig.update_layout(title=f\"{service} distribution\",barmode='group')\n",
    "            fig.show()\n",
    "\n",
    "            #Feature distribution wrt Churn\n",
    "            df2 = data.groupby([service, 'Churn']).count()['customerID'].unstack()\n",
    "\n",
    "            fig = go.Figure(data=[\n",
    "                go.Bar(name='Churn-Yes', x=df2.index, y=df2['Yes'], marker_color='indianred' ),\n",
    "                go.Bar(name='Churn-No', x=df2.index, y=df2['No'], marker_color='seagreen')])\n",
    "\n",
    "            fig.update_layout( title=f\"{service} distribution wrt Churn\",barmode='group')\n",
    "\n",
    "            fig.show() \n",
    "\n",
    "payment_services(data)\n",
    "\n",
    "People opting for automated payment methods and those that prefer paperless billings  are likely to churn \n",
    "\n",
    "### Handling irrelevant/redundant features\n",
    "\n",
    "It is ideal to remove duplicate features (redundant) and irrelevant features that contain no information that is useful for the data mining task at hand.\n",
    "\n",
    "Here, the `customerID` doesn't affect the customers' churn rate as it solely use for unique identification purposes only.\n",
    "\n",
    "# Dropping the customerID feature\n",
    "data.drop(columns=['customerID'], inplace=True)\n",
    "\n",
    "### Handling categorical data\n",
    "Typically, any structured dataset includes multiple columns – a combination of numerical as well as categorical variables. A machine can only understand the numbers. It cannot understand the text. That’s essentially the case with Machine Learning algorithms too.\n",
    "\n",
    "Categorical features are features which can take on values from a limited set of values. Examples include:\n",
    "\n",
    "- Levels of heat (Hot, Hotter, Hottest, Android Studio)\n",
    "- Types of food (Nigerian Jollof, bread, Eba, Beans)\n",
    "\n",
    "\n",
    "We have two types of categorical data\n",
    "1. Ordinal categorical data:  Features have natural, ordered category. (One class is higher than another).\n",
    "2. Non Ordinal categorical data: Features have no order. (No class is higher than the other).\n",
    "\n",
    "\n",
    "**ORDINAL ENCODING:**\n",
    "\n",
    "Assign values to each unique category taking into account their order.\n",
    "\n",
    "\n",
    "**NON-ORDINAL ENCODING:**\n",
    "\n",
    "A quick logic for remembering which encoding scheme to use\n",
    "\n",
    "```python\n",
    "\n",
    "if len(unique classes) is large:\n",
    "\n",
    "       Use (Label Encoding | Binary Encoding | Hash Encoding\n",
    "else:\n",
    "       Use (One Hot Encoding)\n",
    "Try:\n",
    "        Target/Mean Encoding \n",
    "\n",
    "        Count Encoding\n",
    "```\n",
    "\n",
    "1. Label Encoding: Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on order.\n",
    "\n",
    "2. Binary Encoding: Binary encoding can be thought of as a hybrid of one-hot and hashing encoders. Binary creates fewer features than one-hot, while preserving some uniqueness of values in the column. It can work well with higher dimensionality ordinal data.\n",
    "\n",
    "3. Hash Encoding: A multivariate hashing implementation with configurable dimensionality/precision.The advantage of this encoder is that it does not maintain a dictionary of observed categories. Consequently, the encoder does not grow in size and accepts new values during data scoring by design.\n",
    "\n",
    "4. One-Hot Encoding: One-Hot Encoding is another popular technique for treating categorical variables. It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature.\n",
    "\n",
    "5. Target/Mean Encoding: For the case of categorical target: features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data. For the case of continuous target: features are replaced with a blend of the expected value of the target given particular categorical value and the expected value of the target over all the training data.\n",
    "\n",
    "\n",
    "To support Pandas DataFrame out of the box, sklearn has a library (Part of the sklearn.contrib class) called Category_encoders to aid thes various categorical encoding scheme. I will be untilizing pandas get_dummies method\n",
    "\n",
    "data.info()\n",
    "\n",
    "cat_cols = data.select_dtypes(include=['object']).columns.to_list()\n",
    "cat_cols.pop()\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(data[col].value_counts())\n",
    "\n",
    "Observe that Name: StreamingMovies is ordinal and the other categorical dataset are nom-ordinal.\n",
    "\n",
    "**Before encoding these categorical features, seperate the target and feature matrix**\n",
    "\n",
    "data['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "data['Churn'].replace(to_replace='No',  value=0, inplace=True)\n",
    "\n",
    "data.head()\n",
    "\n",
    "#target \n",
    "y = data['Churn']\n",
    "x = data.drop(columns=['Churn'])\n",
    "\n",
    "# The contract feature is ordinal\n",
    "x['Contract'].replace(to_replace='Month-to-month', value=1, inplace=True)\n",
    "x['Contract'].replace(to_replace='One year',  value=2, inplace=True)\n",
    "x['Contract'].replace(to_replace='Two year',  value=3, inplace=True)\n",
    "\n",
    "x['Contract'].values\n",
    "\n",
    "#We need to convert the remaining non-ordinal categorical data using one hot encoding\n",
    "ml_dummies = pd.get_dummies(x, drop_first=True)\n",
    "# To avoid dummy trap, we have to drop one dummy column for which unique feature\n",
    "ml_dummies.fillna(value=0, inplace=True)\n",
    "ml_dummies.head()\n",
    "\n",
    "# Alternatively\n",
    "# import category_encoders as ce\n",
    "# one_hot_enc = ce.OneHotEncoder(cols=cat_cols)\n",
    "# ml_dummies = one_hot_enc.fit_transform(x)\n",
    "# columns_to_drop = ['gender_2','SeniorCitizen_2','Partner_2', 'Dependents_2','PhoneService_2', 'MultipleLines_3', 'InternetService_3',\n",
    "#                    'OnlineSecurity_3', 'OnlineBackup_3', 'DeviceProtection_3', 'TechSupport_3', 'StreamingMovies_3', 'Contract_3', \n",
    "#                     'PaperlessBilling_2', 'PaymentMethod_4'\n",
    "#                   ]\n",
    "# ml_dummies.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# # Add a random column to the dataframe\n",
    "# ml_dummies['---randomColumn---'] = np.random.randint(0,1000, size=len(ml_dummies))\n",
    "\n",
    "ml_dummies.shape\n",
    "\n",
    "x = ml_dummies\n",
    "\n",
    "x.shape\n",
    "\n",
    "cols = x.columns\n",
    "\n",
    "#### Correlation matrix\n",
    "\n",
    "Below chart helps in identifying the right features/variables that are correlated for model building\n",
    "\n",
    "correlation = x.corr()\n",
    "#tick labels\n",
    "matrix_cols = correlation.columns.tolist()\n",
    "#convert to array\n",
    "corr_array = np.array(correlation)\n",
    "\n",
    "#Plotting\n",
    "trace = go.Heatmap(z = corr_array,\n",
    "                   x = matrix_cols,\n",
    "                   y = matrix_cols,\n",
    "                   colorscale = \"Viridis\",\n",
    "                   colorbar = dict(title = \"Pearson Correlation coefficients\", titleside = \"right\"),\n",
    "                  )\n",
    "layout = go.Layout(dict(title = \"Correlation matrix\",\n",
    "                        autosize = False,\n",
    "                        height = 720,\n",
    "                        width = 800,\n",
    "                        margin = dict(r = 0, l = 210, t = 25, b = 210),\n",
    "                        yaxis = dict(tickfont = dict(size = 9)),\n",
    "                        xaxis = dict(tickfont = dict(size = 9))\n",
    "                       )\n",
    "                  )\n",
    "trace = [trace]\n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms.\n",
    "\n",
    "# # Calculate features\n",
    "# x['total_charges_to_tenure_ratio'] = x['TotalCharges'] / x['tenure']\n",
    "# x['monthly_charges_diff'] = x['MonthlyCharges'] - x['total_charges_to_tenure_ratio']\n",
    "# # x.fillna(x.mean())\n",
    "# # x = np.nan_to_num(x)\n",
    "\n",
    "### Feature Scaling\n",
    "\n",
    "Feature Scaling helps change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.\n",
    "\n",
    "This is very important for distant based models like KNN and also helps speed up trainng in Neural Nets.\n",
    "\n",
    "Some Normalization functions available in sklearn:\n",
    "\n",
    "    1. StandardScaler: Standardize features by removing the mean and scaling to unit variance.\n",
    "    2. RobustScaler: Scale features using statistics that are robust to outliers.\n",
    "    3. MinMaxScaler: Transforms features by scaling each feature to a given range. Range (Depends on You)\n",
    "\n",
    "#Use standard scaler\n",
    "#Note: If you have a train and test set, NEVER FIT STANDARD SCALER TO THE TEST/VAL SET! \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "scaled_data = sc.fit_transform(x)\n",
    "\n",
    "# #Use Robust scaler\n",
    "# #Note: If you have a train and test set, NEVER FIT ROBUST SCALER TO THE TEST/VAL SET! \n",
    "\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# robsc = RobustScaler()\n",
    "# scaled_data = robsc.fit_transform(x)\n",
    "\n",
    "# #Use MinMaxScaler scaler\n",
    "\n",
    "# #Note: If you have a train and test set, NEVER FIT MINMAXSCALER TO THE TEST/VAL SET! \n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# minsc = MinMaxScaler(feature_range=(0,1))\n",
    "# scaled_data = minsc.fit_transform(x)\n",
    "\n",
    "scaled_data\n",
    "\n",
    "scaled_data = pd.DataFrame(data=scaled_data, columns=cols)\n",
    "\n",
    "scaled_data.head()\n",
    "\n",
    "### Splitting the dataset\n",
    "\n",
    "# Create Train & Test Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Let's take a break\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "\n",
    "<table>\n",
    "    <th> <img src=\"../images/break.jpg\" style=\"width: 1000px;\"> </th>\n",
    "    <th><h1>I hope you are enjoying the session?</h1></th>\n",
    " </table>\n",
    "\n",
    "### Model Building\n",
    "\n",
    "Building machine learning models that have the ability to generalize well on future data requires thoughtful consideration of the data at hand and of assumptions about various available training algorithms. Ultimate evaluation of a machine learning model’s quality requires an appropriate selection and interpretation of assessment criteria.\n",
    "\n",
    "Once you’ve defined your problem, prepared your data, evaluation criteria and features it’s time to model.\n",
    "\n",
    "Modelling breaks into three parts,\n",
    "1. choosing a model,\n",
    "2. improving a model,\n",
    "3. comparing it with others.\n",
    "\n",
    "For the scope of this session, we will develop some predictive models using the following Machine Learning Algorithms:\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- Support Vector Machine (SVM)\n",
    "- Gaussian Naive Bayes\n",
    "- KNN Classifier\n",
    "\n",
    "\n",
    "Then try out some boosting algorithms like:\n",
    "\n",
    "\n",
    "- Lightgbm\n",
    "- XGBoost\n",
    "- AdaBoost\n",
    "- GradientBoost\n",
    "\n",
    "Improving the model will be carried out by\n",
    "- resampling technique,\n",
    "- hyperparameter tuning and \n",
    "- feature selection\n",
    "\n",
    "Models comparison will be made possible by the following performance metrics:\n",
    "- Accuracy score: Accuracy is the simple ratio between the number of correctly classified points to the total number of points. Accuracy is simple to calculate but has its own disadvantages.\n",
    "      Limitations of accuracy\n",
    "\n",
    "    - If the data set is highly imbalanced, and the model classifies all the data points as the majority class data points, the accuracy will be high. This makes accuracy not a reliable performance metric for imbalanced data.\n",
    "    - From accuracy, the probability of the predictions of the model can be derived. So from accuracy, we can not measure how good the predictions of the model are. \n",
    "***\n",
    "- roc_auc_score: The area under the ROC curve (ROC AUC) is the single-valued metric used for evaluating the performance. The higher the AUC, the better the performance of the model at distinguishing between the classes. In general, an AUC of 0.5 suggests no discrimination, a value between 0.5–0.7 is acceptable and anything above 0.7 is good-to-go-model. However, medical diagnosis models, usually AUC of 0.95 or more is considered to be good-to-go-model.\n",
    "\n",
    "When to use ROC?\n",
    "\n",
    "- ROC curves are widely used to compare and evaluate different classification algorithms.\n",
    "- ROC curve is widely used when the dataset is imbalanced.\n",
    "\n",
    "\n",
    "# importing all the algorithms and metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def churn_prediction(algorithm, training_x, testing_x, training_y, testing_y):\n",
    "    algorithm.fit(training_x, training_y)\n",
    "    predictions = algorithm.predict(testing_x)\n",
    "    \n",
    "    print('Algorithm:', type(algorithm).__name__)\n",
    "    print(\"\\nAccuracy Score:\", accuracy_score(testing_y, predictions))\n",
    "    model_roc_auc = roc_auc_score(testing_y, predictions) \n",
    "    print(\"\\nArea under curve:\", model_roc_auc,\"\\n\")\n",
    "    \n",
    "\n",
    "### Baseline model\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 9, random_state = 123,\n",
    "                                       splitter = \"best\", criterion = \"gini\")\n",
    "\n",
    "churn_prediction(decision_tree, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Random Forest\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "churn_prediction(rfc, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "churn_prediction(knn, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#support vector classifier using non-linear hyper plane (\"rbf\")\n",
    "svc_rbf  = SVC(C=1.0, kernel='rbf', gamma=0.1, probability=True)   \n",
    "\n",
    "churn_prediction(svc_rbf, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#Support vector classifier using linear hyper plane\n",
    "svc_lin  = SVC(C=1.0, kernel='linear',  gamma=0.1,probability=True)\n",
    "\n",
    "churn_prediction(svc_lin, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Logistic Regression       \n",
    "logit = LogisticRegression()\n",
    "\n",
    "churn_prediction(logit, X_train, X_test, y_train, y_test)\n",
    "\n",
    "#  LightGBM Classifier\n",
    "lgbmc = LGBMClassifier()\n",
    "\n",
    "churn_prediction(lgbmc, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgc = XGBClassifier()\n",
    "\n",
    "churn_prediction(xgc,  X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Adaboost\n",
    "adac = AdaBoostClassifier()\n",
    "\n",
    "churn_prediction(adac,  X_train, X_test, y_train, y_test)\n",
    "\n",
    "#GradientBoost\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "churn_prediction(gbc,  X_train, X_test, y_train, y_test)\n",
    "\n",
    "#putting all the model names, model classes and the used columns in a dictionary\n",
    "models = {'Decision Tree(Baseline)': [decision_tree],\n",
    "          'Random Forest': [rfc],\n",
    "          'KNN Classifier': [knn], \n",
    "          'SVM (rbf)': [svc_rbf], \n",
    "          'SVM (linear)': [svc_lin], \n",
    "          'Logistic': [logit],\n",
    "          'LGBM Classifier': [lgbmc],\n",
    "          'XGBoost Classifier': [xgc], \n",
    "          'AdaBoost': [adac], \n",
    "          'GradientBoost': [gbc], \n",
    "         }\n",
    "\n",
    "#gives model report in dataframe\n",
    "def model_report(model, training_x, testing_x, training_y, testing_y, name):\n",
    "    model = model.fit(training_x, training_y)\n",
    "    predictions = model.predict(testing_x)\n",
    "    accuracy = accuracy_score(testing_y, predictions)\n",
    "    roc_auc = roc_auc_score(testing_y, predictions)\n",
    "   \n",
    "    \n",
    "    df = pd.DataFrame({\"Model\"           : [name],\n",
    "                       \"Accuracy\"        : [accuracy],\n",
    "                       \"Roc_auc\"         : [roc_auc]\n",
    "                      })\n",
    "    return df\n",
    "\n",
    "#outputs for all models over the training dataset\n",
    "model_performances_train = pd.DataFrame() \n",
    "\n",
    "for name in models:\n",
    "     model_performances_train = model_performances_train.append(model_report(models[name][0],X_train, \n",
    "                                                                                X_test, \n",
    "                                                                                y_train, y_test, name), ignore_index=True)\n",
    "        \n",
    "table_train = ff.create_table(np.round(model_performances_train, 4))\n",
    "py.iplot(table_train)\n",
    "\n",
    "It can observed that the Naive Bayes had the highest ROC_AUC score, let's see if we can improve on that.\n",
    "\n",
    "**Let's try to tune our hyperparameters**\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "lgt_bst = LogisticRegression()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'C': loguniform(1e-5, 100),\n",
    "}\n",
    "\n",
    "\n",
    "model = RandomizedSearchCV(lgt_bst, param_grid, n_iter=500, scoring='roc_auc', n_jobs=-1, cv=cv, random_state=1)\n",
    "search = model.fit(x, y)\n",
    "search.best_params_\n",
    "\n",
    "best_model = LogisticRegression(random_state=1, C=0.008301451461243866, penalty='none', solver='newton-cg' )\n",
    "\n",
    "#  LightGBM Classifier\n",
    "lgbmc = LGBMClassifier()\n",
    "\n",
    "churn_prediction(best_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "Here brings the concepts of underfitting and overfitting...\n",
    "\n",
    "***\n",
    "# What Next?\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
